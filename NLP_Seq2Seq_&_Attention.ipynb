{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMfQZXuTa53Vz5vstzntvUT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nuvita97/NLP-Machine-Translation/blob/main/NLP_Seq2Seq_%26_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9X7KOiYf_uPy",
        "outputId": "d70db142-7ab9-4949-dbb8-8900786ca56b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP-Machine-Translation'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 21 (delta 7), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (21/21), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/nuvita97/NLP-Machine-Translation.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "import io\n",
        "import string\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "p-RvauHtA-5M"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'NLP-Machine-Translation'\n",
        "os.chdir(path)\n",
        "os.listdir()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sETFUhGI_2Mk",
        "outputId": "71fb9741-78b9-45e0-e6b9-0b4b1fd7630c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['README.md',\n",
              " '.git',\n",
              " 'train.vi.txt',\n",
              " 'NLP_Seq2Seq_&_Attention.ipynb',\n",
              " 'train.en.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_filename = \"train.en.txt\"\n",
        "vi_filename = \"train.vi.txt\"\n",
        "\n",
        "raw_en_lines = open(en_filename, encoding='utf-8').read().strip().split(\"\\n\")\n",
        "raw_vi_lines = open(vi_filename, encoding='utf-8').read().strip().split(\"\\n\")\n",
        "\n",
        "print(len(raw_en_lines))\n",
        "print(len(raw_vi_lines))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umCXVI4T_9ud",
        "outputId": "b9358e2d-9842-4db8-cfc1-d2ae6bacc931"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "133317\n",
            "133317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(raw_en_lines[:3])\n",
        "display(raw_vi_lines[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "pvNqPzpXMGxY",
        "outputId": "f2dcb692-fc3f-4107-b787-2d67f7e979c0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['Rachel Pike : The science behind a climate headline',\n",
              " 'In 4 minutes , atmospheric chemist Rachel Pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a risky flight over the rainforest in pursuit of data on a key molecule .',\n",
              " 'I &apos;d like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper .']"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['Khoa học đằng sau một tiêu đề về khí hậu',\n",
              " 'Trong 4 phút , chuyên gia hoá học khí quyển Rachel Pike giới thiệu sơ lược về những nỗ lực khoa học miệt mài đằng sau những tiêu đề táo bạo về biến đổi khí hậu , cùng với đoàn nghiên cứu của mình -- hàng ngàn người đã cống hiến cho dự án này -- một chuyến bay mạo hiểm qua rừng già để tìm kiếm thông tin về một phân tử then chốt .',\n",
              " 'Tôi muốn cho các bạn biết về sự to lớn của những nỗ lực khoa học đã góp phần làm nên các dòng tít bạn thường thấy trên báo .']"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Punctuation : \", string.punctuation )\n",
        "print(\"Digits : \", string.digits )\n",
        "\n",
        "exclude = list(string.punctuation) + list(string.digits)\n",
        "print(\"Exclude : \", exclude)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTRXvd5fMMLR",
        "outputId": "7e50f7f5-044a-4146-9bf3-3a0864e896b7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuation :  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "Digits :  0123456789\n",
            "Exclude :  ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(sentence):\n",
        "  sent = sentence.lower()\n",
        "  sent = sent.strip()\n",
        "  sent = re.sub(\"'\", \" \", sent)\n",
        "  sent = re.sub(\"\\s+\", \" \", sent)\n",
        "  sent = ''.join([char for char in sent if char not in exclude])\n",
        "  sent = \"<start> \" + sent + \" <end>\"\n",
        "  return sent\n",
        "\n",
        "preprocess(\"I go to school\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AnseG9llAAzp",
        "outputId": "0b1fbdb2-0841-414b-dece-25895504cfeb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i go to school <end>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_lines = []\n",
        "vi_lines = []\n",
        "min_len, max_len = 10, 14\n",
        "\n",
        "for eline, vline in zip(raw_en_lines, raw_vi_lines):\n",
        "  eline = preprocess(eline)\n",
        "  vline = preprocess(vline)\n",
        "  if min_len < len(eline.split(\" \")) < max_len and min_len < len(vline.split(\" \")) < max_len:\n",
        "    en_lines.append(eline)\n",
        "    vi_lines.append(vline)\n",
        "        \n",
        "print(len(en_lines))\n",
        "print(len(vi_lines))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjVf28tGAD_6",
        "outputId": "73721222-7620-4005-a153-b779ba702d91"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6016\n",
            "6016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Language():\n",
        "  def __init__(self, lines):\n",
        "    self.lines = lines\n",
        "    self.word2id = {}\n",
        "    self.id2word = {}\n",
        "    self.vocab = set()\n",
        "    self.max_len = 0\n",
        "    self.min_len = 0\n",
        "    self.vocab_size = 0\n",
        "    self.init_language_params()\n",
        "\n",
        "  def init_language_params(self):\n",
        "    for line in self.lines:\n",
        "        self.vocab.update(line.split(\" \"))\n",
        "    self.word2id['<pad>'] = 0\n",
        "    for id, word in enumerate(self.vocab):\n",
        "        self.word2id[word] = id + 1\n",
        "    for word, id in self.word2id.items():\n",
        "        self.id2word[id] = word\n",
        "    self.max_len = max([len(line.split(\" \")) for line in self.lines])\n",
        "    self.min_len = min([len(line.split(\" \")) for line in self.lines])\n",
        "    self.vocab_size = len(self.vocab) + 1\n",
        "          \n",
        "  def sentence_to_vector(self, sent):\n",
        "    return np.array([self.word2id[word] for word in sent.split(\" \")])\n",
        "          \n",
        "  def vector_to_sentence(self, vector):\n",
        "    return \" \".join([self.id2word[id] for id in vector])"
      ],
      "metadata": {
        "id": "6QhORi1BAGZe"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_lang = Language(en_lines)\n",
        "tar_lang = Language(vi_lines)\n",
        "print(inp_lang.max_len, inp_lang.min_len)\n",
        "print(tar_lang.max_len, tar_lang.min_len)\n",
        "\n",
        "inp_vector = [inp_lang.sentence_to_vector(line) for line in inp_lang.lines]\n",
        "tar_vector = [tar_lang.sentence_to_vector(line) for line in tar_lang.lines]\n",
        "\n",
        "inp_tensor = tf.keras.preprocessing.sequence.pad_sequences(inp_vector, inp_lang.max_len, padding='post')\n",
        "tar_tensor = tf.keras.preprocessing.sequence.pad_sequences(tar_vector, tar_lang.max_len, padding='post')\n",
        "print(inp_tensor.shape, tar_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEiG8USaAHVL",
        "outputId": "0b7bdf78-dcf7-4493-c232-a655c83581ea"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13 11\n",
            "13 11\n",
            "(6016, 13) (6016, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(inp_tensor, tar_tensor, test_size=0.1)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = x_train.shape[0]\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "hidden_unit = 1024\n",
        "embedding_size = 256\n",
        "print(BUFFER_SIZE)\n",
        "\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "dataset = dataset.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McCgry6LAMoZ",
        "outputId": "49122e23-6102-4861-ea83-719fa584daf1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_x, tmp_y = next(iter(dataset))\n",
        "print(tmp_x.shape)\n",
        "print(tmp_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5sfcK20AOKN",
        "outputId": "40ce6713-9cf6-40ee-fce7-65aa691f20d4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 13)\n",
            "(32, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encode(tf.keras.Model):\n",
        "  def __init__(self, embedding_size, vocab_size, hidden_units):\n",
        "    super(Encode, self).__init__()\n",
        "    self.Embedding = tf.keras.layers.Embedding(vocab_size,embedding_size)\n",
        "    self.GRU = tf.keras.layers.GRU(\n",
        "      hidden_units,\n",
        "      return_sequences=True,\n",
        "      return_state=True,\n",
        "      recurrent_initializer='glorot_uniform')\n",
        "    self.hidden_units = hidden_units\n",
        "      \n",
        "  def call(self, x, hidden_state):\n",
        "    try:\n",
        "      x = self.Embedding(x)\n",
        "    except:\n",
        "      print(x, print(inp_lang.vocab_size))          \n",
        "    outputs, last_state = self.GRU(x, hidden_state)\n",
        "    return outputs, last_state\n",
        "  \n",
        "  def init_hidden_state(self, batch_size):\n",
        "    return tf.zeros([batch_size, self.hidden_units])"
      ],
      "metadata": {
        "id": "C-LbUjTTAOiv"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encode(embedding_size, inp_lang.vocab_size, hidden_unit)\n",
        "hidden_state = encoder.init_hidden_state(BATCH_SIZE)\n",
        "tmp_outputs, last_state = encoder(tmp_x, hidden_state)\n",
        "print(tmp_outputs.shape)\n",
        "print(last_state.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_MUuf8mAQRN",
        "outputId": "0cfc4665-bbd2-4842-ee28-3ac1832d6668"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 13, 1024)\n",
            "(32, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(tf.keras.Model):\n",
        "  def __init__(self, hidden_units):\n",
        "    super(Attention, self).__init__()\n",
        "    self.W_out_encode = tf.keras.layers.Dense(hidden_unit)\n",
        "    self.W_state = tf.keras.layers.Dense(hidden_unit)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "      \n",
        "  def call(self, encode_outs, pre_state):\n",
        "    pre_state = tf.expand_dims(pre_state, axis=1)\n",
        "    pre_state = self.W_state(pre_state)\n",
        "    encode_outs = self.W_out_encode(encode_outs)\n",
        "    score = self.V(tf.nn.tanh(pre_state + encode_outs))\n",
        "    score = tf.nn.softmax(score, axis=1)\n",
        "    context_vector = score*encode_outs\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    return context_vector, score"
      ],
      "metadata": {
        "id": "P4yXuT4UASmZ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention = Attention(hidden_unit)\n",
        "context_vector, attention_weight = attention(tmp_outputs, last_state)\n",
        "print(context_vector.shape, attention_weight.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XQ29FS8ATGN",
        "outputId": "ba780c08-647f-436d-b218-f325e7d63b7a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 1024) (32, 13, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decode(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_size, hidden_units):\n",
        "    super(Decode, self).__init__()\n",
        "    self.hidden_units = hidden_units\n",
        "    self.Embedding = tf.keras.layers.Embedding(vocab_size,embedding_size)\n",
        "    self.Attention = Attention(hidden_units)\n",
        "    self.GRU = tf.keras.layers.GRU(\n",
        "        hidden_units,\n",
        "        return_sequences=True,\n",
        "        return_state=True,\n",
        "        recurrent_initializer='glorot_uniform'\n",
        "    )\n",
        "    self.Fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "  def call(self, x, encode_outs, pre_state):\n",
        "    x = tf.expand_dims(x, axis=1)\n",
        "    try:\n",
        "        x = self.Embedding(x)\n",
        "    except:\n",
        "        print(x, print(tar_lang.vocab_size))          \n",
        "    context_vector, attention_weight = self.Attention(encode_outs, pre_state)\n",
        "    context_vector = tf.expand_dims(context_vector, axis=1)\n",
        "    gru_inp = tf.concat([x, context_vector], axis=-1)\n",
        "    out_gru, state = self.GRU(gru_inp)\n",
        "    out_gru = tf.reshape(out_gru, (-1, out_gru.shape[2]))\n",
        "    return self.Fc(out_gru), state\n",
        "  \n",
        "  \n",
        "decode = Decode(tar_lang.vocab_size, embedding_size, hidden_unit)\n",
        "print(last_state.shape, tmp_outputs.shape, tmp_y[:, 0].shape)\n",
        "decode_out, state = decode(tmp_y[:, 0], tmp_outputs, last_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrUa3RWUAVwg",
        "outputId": "07b6819b-49cd-4a2b-f8eb-ea09ea2a5094"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 1024) (32, 13, 1024) (32,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = 1 - np.equal(real, 0)\n",
        "  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "id": "ffLYc-5dAXjz"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "optimizer = tf.optimizers.Adam()\n",
        "encoder = Encode(embedding_size, vocab_size=inp_lang.vocab_size, hidden_units=hidden_unit)\n",
        "decoder = Decode(vocab_size=tar_lang.vocab_size, embedding_size=embedding_size, hidden_units=hidden_unit)"
      ],
      "metadata": {
        "id": "SK6yxJYjAZhs"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  total_loss = 0\n",
        "  for batch_id, (x, y) in enumerate(dataset.take(N_BATCH)):\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "      first_state = encoder.init_hidden_state(batch_size=BATCH_SIZE)\n",
        "      encode_outs, last_state = encoder(x, first_state)\n",
        "      decode_state = last_state\n",
        "      decode_input = [tar_lang.word2id[\"<start>\"]]*BATCH_SIZE\n",
        "      \n",
        "      for i in range(1, y.shape[1]):\n",
        "          decode_out, decode_state = decoder(decode_input, encode_outs, decode_state)\n",
        "          loss += loss_function(y[:, i], decode_out)\n",
        "          decode_input = y[:, i]\n",
        "          \n",
        "      train_vars = encoder.trainable_variables + decoder.trainable_variables\n",
        "      grads = tape.gradient(loss, train_vars)\n",
        "      optimizer.apply_gradients(zip(grads, train_vars))\n",
        "    total_loss += loss\n",
        "  print(total_loss.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU9NQTLnAbdE",
        "outputId": "585c4a8a-c2c4-465a-b0f0-356661d3777d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10592.643\n",
            "9103.253\n",
            "8175.322\n",
            "7314.9844\n",
            "6507.0796\n",
            "5784.626\n",
            "5195.4585\n",
            "4580.087\n",
            "3982.1167\n",
            "3466.7974\n",
            "3082.3875\n",
            "2742.4727\n",
            "2393.151\n",
            "2081.3403\n",
            "1893.7026\n",
            "1611.1245\n",
            "1405.6572\n",
            "1249.9858\n",
            "1074.8694\n",
            "927.9366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(inputs):\n",
        "  print(inp_lang.vector_to_sentence(inputs[0].numpy()))\n",
        "  result = ''\n",
        "\n",
        "  hidden = encoder.init_hidden_state(batch_size=1)\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "  dec_hidden = enc_hidden\n",
        "  print(enc_out.shape, dec_hidden.shape)\n",
        "  \n",
        "  dec_input = [tar_lang.word2id['<start>']]\n",
        "  for t in range(tar_lang.max_len):\n",
        "      predictions, dec_hidden = decoder(dec_input, enc_out, dec_hidden)\n",
        "      predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "      result += tar_lang.id2word[predicted_id] + ' '\n",
        "      dec_input = [predicted_id]\n",
        "  return result\n",
        "  \n",
        "for inp, tar in dataset.take(N_BATCH):\n",
        "  print(translate(inp[5:6,:]))\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0L-qY9jIAbV0",
        "outputId": "8fb6ad4a-e823-40eb-d8a0-a39130d96a34"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> but this has come as a great boon for them  <end>\n",
            "(1, 13, 1024) (1, 1024)\n",
            "nhưng điều này là một món quà tuyệt nhất cho chúng  <end> \n"
          ]
        }
      ]
    }
  ]
}